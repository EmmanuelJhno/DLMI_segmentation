{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time \n",
    "import copy\n",
    "import argparse\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import tqdm\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from models import get_model\n",
    "from data_loader.LiTS import LiTSDataset\n",
    "from utils.train import ObjFromDict\n",
    "\n",
    "\n",
    "def load_model(run_dir, metric='validation_dice'): \n",
    "    with open(os.path.join(run_dir,'config.json')) as json_file:\n",
    "        config = json.load(json_file)\n",
    "    config = ObjFromDict(config)\n",
    "    model = get_model(config.model)\n",
    "    checkpoint_path = os.path.join(run_dir, 'best_{}.pth'.format(metric)) \n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    return model, config\n",
    "\n",
    "def compute_dice(gt, pred): \n",
    "    eps = 1e-5\n",
    "    intersection = np.sum(gt * pred)\n",
    "    return ((2*  intersection) + eps)/ (eps + np.sum(gt+pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_scale not specified in config, setting to default 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raubyb/LiTS/DLMI_segmentation/models/unet.py:85: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
      "/home/raubyb/LiTS/DLMI_segmentation/models/unet.py:89: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(m.weight.data, 1.0, 0.02)\n",
      "/home/raubyb/LiTS/DLMI_segmentation/models/unet.py:90: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(m.bias.data, 0.0)\n"
     ]
    }
   ],
   "source": [
    "run_dir = 'runs/2020-03-28_17h33min'\n",
    "model, config = load_model(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device='cuda:0'\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path  /home/raubyb/LiTS\n"
     ]
    }
   ],
   "source": [
    "data_path = config.dataset.root\n",
    "print('data_path ', data_path)\n",
    "\n",
    "# fix the seed for the split\n",
    "split_seed = 0 \n",
    "np.random.seed(split_seed)\n",
    "\n",
    "image_dir = os.listdir(os.path.join(data_path,'Training Batch 1')) + os.listdir(os.path.join(data_path,'Training Batch 2'))\n",
    "all_indexes = [ int(file_name[7:-4]) for file_name in image_dir if 'volume' in file_name]\n",
    "split = np.random.permutation(all_indexes)\n",
    "n_train, n_val, n_test = int(0.8 * len(split)), int(0.1 * len(split)), int(0.1 * len(split))\n",
    "\n",
    "train = split[: n_train]\n",
    "val = split[n_train : n_train+n_val]\n",
    "test = split[n_train + n_val :]\n",
    "\n",
    "\n",
    "# Setup Data Loader\n",
    "train_dataset = LiTSDataset(data_path, train, augment=True, no_tumor=True)\n",
    "val_dataset = LiTSDataset(data_path, val, no_tumor=True)\n",
    "test_dataset = LiTSDataset(data_path, test, no_tumor=True)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, num_workers=config.dataset.num_workers, batch_size=config.training.batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, num_workers=config.dataset.num_workers, batch_size=config.training.batch_size, shuffle=False)\n",
    "test_dataloader  = DataLoader(dataset=test_dataset,  num_workers=config.dataset.num_workers, batch_size=config.training.batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "2it [04:50, 171.98s/it]"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "test_dataset = LiTSDataset(data_path, test, no_tumor=True, inference_mode=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, num_workers=config.dataset.num_workers, batch_size=batch_size, shuffle=False)\n",
    "dices = []\n",
    "# train_dataloader = DataLoader(dataset=train_dataset, num_workers=config.dataset.num_workers, batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in tqdm.tqdm(enumerate(test_dataloader)):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            for i in range(batch_size):\n",
    "                img = sitk.ReadImage(target['original_image_path'][i])\n",
    "                normalization_transform = test_dataset.get_normalization_transform(img)\n",
    "                inv_normalization_transform = normalization_transform.GetInverse()\n",
    "                out_mask = np.round(output.cpu().numpy()[0,1,:,:,:])\n",
    "\n",
    "                bb = target['bounding_box'].cpu().numpy()[0]\n",
    "\n",
    "                ref_img = sitk.Resample(img, test_dataset.reference_image, normalization_transform)\n",
    "\n",
    "                big_mask_numpy = sitk.GetArrayFromImage(ref_img)\n",
    "                big_mask_numpy[:,:,:] = 0\n",
    "                big_mask_numpy[bb[0]:bb[1],bb[2]:bb[3],bb[4]:bb[5]] = out_mask\n",
    "\n",
    "                out_mask_image = sitk.GetImageFromArray(big_mask_numpy)\n",
    "                out_mask_image.SetOrigin(ref_img.GetOrigin())\n",
    "                out_mask_image.SetDirection(ref_img.GetDirection())\n",
    "                out_mask_image.SetSpacing(ref_img.GetSpacing())\n",
    "                out_mask_image_original_space = sitk.Resample(out_mask_image, img, inv_normalization_transform, sitk.sitkNearestNeighbor)\n",
    "                original_mask = sitk.ReadImage(target['original_mask_path'][i])\n",
    "                original_mask_array = np.clip(sitk.GetArrayFromImage(original_mask),0,1)\n",
    "                out_mask_image_original_space_array = np.clip(sitk.GetArrayFromImage(out_mask_image_original_space),0,1)\n",
    "                dices.append(compute_dice(out_mask_image_original_space_array, original_mask_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(dices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "val_dataset = LiTSDataset(data_path, val, no_tumor=True, inference_mode=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, num_workers=config.dataset.num_workers, batch_size=batch_size, shuffle=False)\n",
    "dices = []\n",
    "# train_dataloader = DataLoader(dataset=train_dataset, num_workers=config.dataset.num_workers, batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in tqdm.tqdm(enumerate(val_dataloader)):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            for i in range(batch_size):\n",
    "                img = sitk.ReadImage(target['original_image_path'][i])\n",
    "                normalization_transform = test_dataset.get_normalization_transform(img)\n",
    "                inv_normalization_transform = normalization_transform.GetInverse()\n",
    "                out_mask = np.round(output.cpu().numpy()[0,1,:,:,:])\n",
    "\n",
    "                bb = target['bounding_box'].cpu().numpy()[0]\n",
    "\n",
    "                ref_img = sitk.Resample(img, test_dataset.reference_image, normalization_transform)\n",
    "\n",
    "                big_mask_numpy = sitk.GetArrayFromImage(ref_img)\n",
    "                big_mask_numpy[:,:,:] = 0\n",
    "                big_mask_numpy[bb[0]:bb[1],bb[2]:bb[3],bb[4]:bb[5]] = out_mask\n",
    "\n",
    "                out_mask_image = sitk.GetImageFromArray(big_mask_numpy)\n",
    "                out_mask_image.SetOrigin(ref_img.GetOrigin())\n",
    "                out_mask_image.SetDirection(ref_img.GetDirection())\n",
    "                out_mask_image.SetSpacing(ref_img.GetSpacing())\n",
    "                out_mask_image_original_space = sitk.Resample(out_mask_image, img, inv_normalization_transform, sitk.sitkNearestNeighbor)\n",
    "                original_mask = sitk.ReadImage(target['original_mask_path'][i])\n",
    "                original_mask_array = np.clip(sitk.GetArrayFromImage(original_mask),0,1)\n",
    "                out_mask_image_original_space_array = np.clip(sitk.GetArrayFromImage(out_mask_image_original_space),0,1)\n",
    "                dices.append(compute_dice(out_mask_image_original_space_array, original_mask_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:11, 11.93s/it]\u001b[A\n",
      "2it [00:21, 11.36s/it]\u001b[A\n",
      "3it [00:29, 10.32s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "val_dataset = LiTSDataset(data_path, val, no_tumor=True, inference_mode=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, num_workers=config.dataset.num_workers, batch_size=batch_size, shuffle=False)\n",
    "dices = []\n",
    "# train_dataloader = DataLoader(dataset=train_dataset, num_workers=config.dataset.num_workers, batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in tqdm.tqdm(enumerate(val_dataloader)):\n",
    "            data = data.to(device)\n",
    "#             output = model(data)\n",
    "            output = target[\"one_hot_target\"]\n",
    "            for i in range(batch_size):\n",
    "                img = sitk.ReadImage(target['original_image_path'][i])\n",
    "                normalization_transform = test_dataset.get_normalization_transform(img)\n",
    "                inv_normalization_transform = normalization_transform.GetInverse()\n",
    "                out_mask = np.round(output.cpu().numpy()[0,1,:,:,:])\n",
    "\n",
    "                bb = target['bounding_box'].cpu().numpy()[0]\n",
    "\n",
    "                ref_img = sitk.Resample(img, test_dataset.reference_image, normalization_transform)\n",
    "\n",
    "                big_mask_numpy = sitk.GetArrayFromImage(ref_img)\n",
    "                big_mask_numpy[:,:,:] = 0\n",
    "                big_mask_numpy[bb[0]:bb[1],bb[2]:bb[3],bb[4]:bb[5]] = out_mask\n",
    "\n",
    "                out_mask_image = sitk.GetImageFromArray(big_mask_numpy)\n",
    "                out_mask_image.SetOrigin(ref_img.GetOrigin())\n",
    "                out_mask_image.SetDirection(ref_img.GetDirection())\n",
    "                out_mask_image.SetSpacing(ref_img.GetSpacing())\n",
    "                out_mask_image_original_space = sitk.Resample(out_mask_image, img, inv_normalization_transform, sitk.sitkNearestNeighbor)\n",
    "                original_mask = sitk.ReadImage(target['original_mask_path'][i])\n",
    "                original_mask_array = np.clip(sitk.GetArrayFromImage(original_mask),0,1)\n",
    "                out_mask_image_original_space_array = np.clip(sitk.GetArrayFromImage(out_mask_image_original_space),0,1)\n",
    "                dice = compute_dice(out_mask_image_original_space_array, original_mask_array)\n",
    "                dices.append(dice)\n",
    "            if dices[-1]>1 or dices[-1]<0.9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(out_mask_image_original_space_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(original_mask_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_mask_image_original_space_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 512, 512)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_mask_image_original_space_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3c2eec7b50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out_mask_image_original_space_array[:,256,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3c2edf12d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(original_mask_array[:,256,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9635634513671141,\n",
       " 0.9734216352324616,\n",
       " 0.9725707337274299,\n",
       " 0.5210175089652535,\n",
       " 0.5961370316465767,\n",
       " 0.977239100075349,\n",
       " 0.9742927557042308,\n",
       " 0.003075100164063234,\n",
       " 0.9547585491495636,\n",
       " 0.9714548070891527,\n",
       " 0.974271647636393,\n",
       " 0.9666417156337599,\n",
       " 0.9657932747052238]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8318644085458899"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = output.cpu().numpy()[0,1,:,:,:]\n",
    "image = data.cpu().numpy()[0,0,:,:,:]\n",
    "volume = np.round(volume)\n",
    "mask = target.cpu().numpy()[0,1,:,:,:]\n",
    "intersection = np.sum(mask*volume)\n",
    "union = np.sum(np.clip(mask+volume,0,1))\n",
    "print(intersection)\n",
    "print(union)\n",
    "print(intersection/union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots(3,1,figsize=(10,10))\n",
    "h,w,d = mask.shape\n",
    "ax[0].imshow(image[:,:,d//2])\n",
    "ax[1].imshow(mask[:,:,d//2])\n",
    "ax[2].imshow(volume[:,:,d//2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRIScoringEnv",
   "language": "python",
   "name": "mriscoringenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
