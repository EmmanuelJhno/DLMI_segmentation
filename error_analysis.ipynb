{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time \n",
    "import copy\n",
    "import argparse\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import tqdm\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from models import get_model\n",
    "from data_loader.LiTS import LiTSDataset\n",
    "from utils.train import ObjFromDict\n",
    "\n",
    "\n",
    "def load_model(run_dir, metric='validation_dice'): \n",
    "    with open(os.path.join(run_dir,'config.json')) as json_file:\n",
    "        config = json.load(json_file)\n",
    "    config = ObjFromDict(config)\n",
    "    model = get_model(config.model)\n",
    "    checkpoint_path = os.path.join(run_dir, 'best_{}.pth'.format(metric)) \n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    return model, config\n",
    "\n",
    "def compute_dice(gt, pred): \n",
    "    eps = 1e-5\n",
    "    intersection = np.sum(gt * pred)\n",
    "    return ((2*  intersection) + eps)/ (eps + np.sum(gt+pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_scale not specified in config, setting to default 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raubyb/LiTS/DLMI_segmentation/models/unet.py:85: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
      "/home/raubyb/LiTS/DLMI_segmentation/models/unet.py:89: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(m.weight.data, 1.0, 0.02)\n",
      "/home/raubyb/LiTS/DLMI_segmentation/models/unet.py:90: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(m.bias.data, 0.0)\n"
     ]
    }
   ],
   "source": [
    "run_dir = 'runs/2020-03-28_17h33min'\n",
    "model, config = load_model(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'\n",
    "# device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path  /home/raubyb/LiTS\n"
     ]
    }
   ],
   "source": [
    "data_path = config.dataset.root\n",
    "print('data_path ', data_path)\n",
    "\n",
    "# fix the seed for the split\n",
    "split_seed = 0 \n",
    "np.random.seed(split_seed)\n",
    "\n",
    "image_dir = os.listdir(os.path.join(data_path,'Training Batch 1')) + os.listdir(os.path.join(data_path,'Training Batch 2'))\n",
    "all_indexes = [ int(file_name[7:-4]) for file_name in image_dir if 'volume' in file_name]\n",
    "split = np.random.permutation(all_indexes)\n",
    "n_train, n_val, n_test = int(0.8 * len(split)), int(0.1 * len(split)), int(0.1 * len(split))\n",
    "\n",
    "train = split[: n_train]\n",
    "val = split[n_train : n_train+n_val]\n",
    "test = split[n_train + n_val :]\n",
    "\n",
    "\n",
    "# Setup Data Loader\n",
    "train_dataset = LiTSDataset(data_path, train, augment=True, no_tumor=True)\n",
    "val_dataset = LiTSDataset(data_path, val, no_tumor=True)\n",
    "test_dataset = LiTSDataset(data_path, test, no_tumor=True)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, num_workers=config.dataset.num_workers, batch_size=config.training.batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, num_workers=config.dataset.num_workers, batch_size=config.training.batch_size, shuffle=False)\n",
    "test_dataloader  = DataLoader(dataset=test_dataset,  num_workers=config.dataset.num_workers, batch_size=config.training.batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "14it [00:55,  3.95s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "test_dataset = LiTSDataset(data_path, test, no_tumor=True, inference_mode=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, num_workers=config.dataset.num_workers, batch_size=batch_size, shuffle=False)\n",
    "dices = []\n",
    "# train_dataloader = DataLoader(dataset=train_dataset, num_workers=config.dataset.num_workers, batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in tqdm.tqdm(enumerate(test_dataloader)):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            for i in range(batch_size):\n",
    "                img = sitk.ReadImage(target['original_image_path'][i])\n",
    "                normalization_transform = test_dataset.get_normalization_transform(img)\n",
    "                inv_normalization_transform = normalization_transform.GetInverse()\n",
    "                out_mask = np.round(output.cpu().numpy()[0,1,:,:,:])\n",
    "\n",
    "                bb = target['bounding_box'].cpu().numpy()[0]\n",
    "\n",
    "                ref_img = sitk.Resample(img, test_dataset.reference_image, normalization_transform)\n",
    "\n",
    "                big_mask_numpy = sitk.GetArrayFromImage(ref_img)\n",
    "                big_mask_numpy[:,:,:] = 0\n",
    "                big_mask_numpy[bb[0]:bb[1],bb[2]:bb[3],bb[4]:bb[5]] = out_mask\n",
    "\n",
    "                out_mask_image = sitk.GetImageFromArray(big_mask_numpy)\n",
    "                out_mask_image.SetOrigin(ref_img.GetOrigin())\n",
    "                out_mask_image.SetDirection(ref_img.GetDirection())\n",
    "                out_mask_image.SetSpacing(ref_img.GetSpacing())\n",
    "                out_mask_image_original_space = sitk.Resample(out_mask_image, img, inv_normalization_transform, sitk.sitkNearestNeighbor)\n",
    "                original_mask = sitk.ReadImage(target['original_mask_path'][i])\n",
    "                original_mask_array = sitk.GetArrayFromImage(original_mask)\n",
    "                out_mask_image_original_space_array = np.clip(sitk.GetArrayFromImage(out_mask_image_original_space),0,1)\n",
    "                dices.append(compute_dice(out_mask_image_original_space_array, original_mask_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9082518674962695,\n",
       " 0.9456416697586999,\n",
       " 0.8690251311078367,\n",
       " 0.9439857988270434,\n",
       " 0.2958480675318669,\n",
       " 0.9290835217616432,\n",
       " 0.7724126878892774,\n",
       " 0.9283849268697714,\n",
       " 0.8161072646933886,\n",
       " 0.9581241187602244,\n",
       " 0.9313413986039893,\n",
       " 0.31074512023752654,\n",
       " 0.4882243699600576,\n",
       " 0.8572997614296676]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7824625503519472\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(dices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:55,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "val_dataset = LiTSDataset(data_path, val, no_tumor=True, inference_mode=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, num_workers=config.dataset.num_workers, batch_size=batch_size, shuffle=False)\n",
    "dices = []\n",
    "# train_dataloader = DataLoader(dataset=train_dataset, num_workers=config.dataset.num_workers, batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in tqdm.tqdm(enumerate(val_dataloader)):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            for i in range(batch_size):\n",
    "                img = sitk.ReadImage(target['original_image_path'][i])\n",
    "                normalization_transform = test_dataset.get_normalization_transform(img)\n",
    "                inv_normalization_transform = normalization_transform.GetInverse()\n",
    "                out_mask = np.round(output.cpu().numpy()[0,1,:,:,:])\n",
    "\n",
    "                bb = target['bounding_box'].cpu().numpy()[0]\n",
    "\n",
    "                ref_img = sitk.Resample(img, test_dataset.reference_image, normalization_transform)\n",
    "\n",
    "                big_mask_numpy = sitk.GetArrayFromImage(ref_img)\n",
    "                big_mask_numpy[:,:,:] = 0\n",
    "                big_mask_numpy[bb[0]:bb[1],bb[2]:bb[3],bb[4]:bb[5]] = out_mask\n",
    "\n",
    "                out_mask_image = sitk.GetImageFromArray(big_mask_numpy)\n",
    "                out_mask_image.SetOrigin(ref_img.GetOrigin())\n",
    "                out_mask_image.SetDirection(ref_img.GetDirection())\n",
    "                out_mask_image.SetSpacing(ref_img.GetSpacing())\n",
    "                out_mask_image_original_space = sitk.Resample(out_mask_image, img, inv_normalization_transform, sitk.sitkNearestNeighbor)\n",
    "                original_mask = sitk.ReadImage(target['original_mask_path'][i])\n",
    "                original_mask_array = sitk.GetArrayFromImage(original_mask)\n",
    "                out_mask_image_original_space_array = np.clip(sitk.GetArrayFromImage(out_mask_image_original_space),0,1)\n",
    "                dices.append(compute_dice(out_mask_image_original_space_array, original_mask_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9163473079039831,\n",
       " 0.9665457074796674,\n",
       " 0.9679224836551575,\n",
       " 0.210158730169825,\n",
       " 0.4513819704331273,\n",
       " 0.9113036318145139,\n",
       " 0.9517790506852649,\n",
       " 0.8297642546379024,\n",
       " 0.9034239510892172,\n",
       " 0.9600934074088654,\n",
       " 0.8825766327302702,\n",
       " 0.8765399457697397,\n",
       " 0.9010985874980277]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9034239510892172"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:47,  3.68s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "val_dataset = LiTSDataset(data_path, val, no_tumor=True, inference_mode=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, num_workers=config.dataset.num_workers, batch_size=batch_size, shuffle=False)\n",
    "dices = []\n",
    "# train_dataloader = DataLoader(dataset=train_dataset, num_workers=config.dataset.num_workers, batch_size=1, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in tqdm.tqdm(enumerate(val_dataloader)):\n",
    "            data = data.to(device)\n",
    "#             output = model(data)\n",
    "            output = target[\"one_hot_target\"]\n",
    "            for i in range(batch_size):\n",
    "                img = sitk.ReadImage(target['original_image_path'][i])\n",
    "                normalization_transform = test_dataset.get_normalization_transform(img)\n",
    "                inv_normalization_transform = normalization_transform.GetInverse()\n",
    "                out_mask = np.round(output.cpu().numpy()[0,1,:,:,:])\n",
    "\n",
    "                bb = target['bounding_box'].cpu().numpy()[0]\n",
    "\n",
    "                ref_img = sitk.Resample(img, test_dataset.reference_image, normalization_transform)\n",
    "\n",
    "                big_mask_numpy = sitk.GetArrayFromImage(ref_img)\n",
    "                big_mask_numpy[:,:,:] = 0\n",
    "                big_mask_numpy[bb[0]:bb[1],bb[2]:bb[3],bb[4]:bb[5]] = out_mask\n",
    "\n",
    "                out_mask_image = sitk.GetImageFromArray(big_mask_numpy)\n",
    "                out_mask_image.SetOrigin(ref_img.GetOrigin())\n",
    "                out_mask_image.SetDirection(ref_img.GetDirection())\n",
    "                out_mask_image.SetSpacing(ref_img.GetSpacing())\n",
    "                out_mask_image_original_space = sitk.Resample(out_mask_image, img, inv_normalization_transform, sitk.sitkNearestNeighbor)\n",
    "                original_mask = sitk.ReadImage(target['original_mask_path'][i])\n",
    "                original_mask_array = sitk.GetArrayFromImage(original_mask)\n",
    "                out_mask_image_original_space_array = np.clip(sitk.GetArrayFromImage(out_mask_image_original_space),0,1)\n",
    "                dices.append(compute_dice(out_mask_image_original_space_array, original_mask_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6060f06b90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out_mask_image_original_space_array[150,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9570940518449507,\n",
       " 1.004649968045515,\n",
       " 0.9937587790696056,\n",
       " 0.2117325411780474,\n",
       " 0.4805301625542665,\n",
       " 0.9473212953290956,\n",
       " 0.9761199813641139,\n",
       " 0.003094506770699072,\n",
       " 0.9062320776107237,\n",
       " 0.9922481321109424,\n",
       " 0.9429071031730156,\n",
       " 0.9364114759640156,\n",
       " 0.9745781782872796]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7943598656386363"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<module 'numpy' from '/home/raubyb/miniconda3/envs/MRIScoringEnv/lib/python3.7/site-packages/numpy/__init__.py'>,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-095295452240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvolume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "volume = output.cpu().numpy()[0,1,:,:,:]\n",
    "image = data.cpu().numpy()[0,0,:,:,:]\n",
    "volume = np.round(volume)\n",
    "mask = target.cpu().numpy()[0,1,:,:,:]\n",
    "intersection = np.sum(mask*volume)\n",
    "union = np.sum(np.clip(mask+volume,0,1))\n",
    "print(intersection)\n",
    "print(union)\n",
    "print(intersection/union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots(3,1,figsize=(10,10))\n",
    "h,w,d = mask.shape\n",
    "ax[0].imshow(image[:,:,d//2])\n",
    "ax[1].imshow(mask[:,:,d//2])\n",
    "ax[2].imshow(volume[:,:,d//2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRIScoringEnv",
   "language": "python",
   "name": "mriscoringenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
